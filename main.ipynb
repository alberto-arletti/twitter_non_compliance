{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d91d25ad",
   "metadata": {},
   "source": [
    "# Processed Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9090d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51193a62",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "da936978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tweets from file \n",
    "df = pd.read_csv('csv_files/processed_anon.csv')\n",
    "# remove retweets\n",
    "db = df[[not i for i in df['is_retweet']]]\n",
    "# add date-like format \n",
    "db['created_at'] = pd.to_datetime(db['created_at'])\n",
    "# remove tweets that got no likes or retweets or comments \n",
    "db = db[(db[['reply_count', 'retweet_count', 'like_count']].sum(axis = 1) != 0)]\n",
    "def is_whitespace_or_url_only(tweet):\n",
    "    # Regex to match URLs\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # Remove all URLs from the tweet\n",
    "    tweet_without_urls = re.sub(url_pattern, '', tweet)\n",
    "    # Check if the remaining tweet is only whitespace\n",
    "    return tweet_without_urls.strip() != ''\n",
    "db = db[db['fulltext'].apply(is_whitespace_or_url_only)]\n",
    "# remove caps which might get identified as a topic \n",
    "db['fulltext'] = db['fulltext'].str.lower()\n",
    "# news outlet list \n",
    "news_outlets = ['il_piccolo', 'infoitinterno', 'TgrRaiFVG', 'Open_gol', 'TRIESTE_news', 'Ansa_Fvg', 'Corriere', \n",
    "                'MediasetTgcom24', 'La7tv', 'localteamtv', 'ComunediTrieste', 'Telequattro', 'messveneto', \n",
    "               'Gazzettino', 'Radio1Rai', 'tempoweb', 'Telefriuli1', 'SkyTG24', 'Agenzia_Ansa', 'ilgiornale',\n",
    "               'fanpage', 'IlFriuli', 'DomaniGiornale', 'DiscoverTrieste', 'ImolaOggi', 'repubblica', 'ilfoglio_it',\n",
    "               'RaCapodistria', 'informatrieste', 'Agenzia_Italia', 'UdineseTV', 'VisioneTv', 'neXtquotidiano', \n",
    "               'fattoquotidiano', 'ilmessaggeroit', 'HuffPostItalia', 'Roma_H_24', 'RaiNews', 'RadioRadioWeb', 'RadioGenova',\n",
    "               'localteamtv', 'byoblu', 'RadioSavana', 'fanpage', 'MediasetTgcom24', 'LaStampa', 'TgLa7',\n",
    "               'Libero_official']\n",
    "# remove from data \n",
    "db = db[[True if not i in news_outlets else False for i in db['username']]]\n",
    "# drop text duplicates \n",
    "db = db.loc[db['fulltext'].drop_duplicates().index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3fc6a",
   "metadata": {},
   "source": [
    "# Topic Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09570786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import openai\n",
    "from bertopic.representation import OpenAI\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead3619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the API key from the text file\n",
    "key_location = '/Users/.../openAI_key.txt'\n",
    "with open(key_location, 'r') as file:\n",
    "    api_key = file.read().strip()  # Remove any leading/trailing whitespace\n",
    "    \n",
    "# Create the OpenAI client with the API key\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "representation_model = OpenAI(client, model=\"gpt-3.5-turbo\", chat=True)\n",
    "# select embedding model \n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "# select BERTopic hyperparameters \n",
    "topic_model = BERTopic(embedding_model=embedding_model,\n",
    "                       representation_model=representation_model, \n",
    "                       verbose=True,\n",
    "                       min_topic_size=70,\n",
    "                       calculate_probabilities = True)\n",
    "# estimate model\n",
    "topics, probs = topic_model.fit_transform(db['fulltext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639add32",
   "metadata": {},
   "source": [
    "# Import meta-topic classification from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0959e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2.service_account import Credentials\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bbf91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load credentials \n",
    "scopes = [\n",
    "    'https://www.googleapis.com/auth/spreadsheets',\n",
    "    'https://www.googleapis.com/auth/drive'\n",
    "]\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    'credentials.json',\n",
    "    scopes=scopes)\n",
    "# access google drive API\n",
    "gc = gspread.authorize(credentials)\n",
    "# open spreadsheet \n",
    "spreadsheet = gc.open(\"twitter_topics_reduced\")\n",
    "# Open the worksheet by its name\n",
    "worksheet = spreadsheet.worksheet('meta')\n",
    "# Get all the data from the worksheet\n",
    "meta_topics = worksheet.get_all_values()\n",
    "# pass to dataframe correctly \n",
    "meta_topics = pd.DataFrame(meta_topics[1:], columns = meta_topics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d36cad2",
   "metadata": {},
   "source": [
    "# Prevalence of toipics depending on opinion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "535d0826",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Meta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m to_mplt \u001b[38;5;241m=\u001b[39m stage1 \u001b[38;5;66;03m# pd.merge(stage1, meta_topics, left_index = True, right_on = 'Topic')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# aggregate for meta topic depending on opinion \u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m to_plot \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mto_mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMeta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39magg(\u001b[38;5;28msum\u001b[39m), to_mplt\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMeta\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39magg(\u001b[38;5;28msum\u001b[39m)], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# drop unwanted topics \u001b[39;00m\n\u001b[1;32m     12\u001b[0m to_plot \u001b[38;5;241m=\u001b[39m to_plot\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/frame.py:8252\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   8250\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8255\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8258\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/groupby/groupby.py:931\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pandas/core/groupby/grouper.py:985\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m    983\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 985\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Meta'"
     ]
    }
   ],
   "source": [
    "# add explicit column for opinion to dataframe (better name)\n",
    "df['opinion'] = df[['support', 'neutral or against']].apply(np.argmax, axis = 1)\n",
    "# group by topic for each opinion\n",
    "topic_opinion = df.groupby('opinion')['Topic'].agg(list).apply(lambda x: pd.Series(x).value_counts()).transpose()[1:]\n",
    "# normalized to sum 1\n",
    "stage1 = topic_opinion.div(topic_opinion.apply(sum))\n",
    "# merge with meta-topics dataframe\n",
    "to_mplt = stage1 # pd.merge(stage1, meta_topics, left_index = True, right_on = 'Topic')\n",
    "# aggregate for meta topic depending on opinion \n",
    "to_plot = pd.concat([to_mplt.groupby('Meta')[0].agg(sum), to_mplt.groupby('Meta')[1].agg(sum)], axis = 1)\n",
    "# drop unwanted topics \n",
    "to_plot = to_plot.drop('noise')\n",
    "# obtain differenced\n",
    "to_plot['diff'] = to_plot[0] - to_plot[1]\n",
    "# sort \n",
    "to_plot = to_plot.sort_values('diff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f1892",
   "metadata": {},
   "source": [
    "# For explorative k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0cc669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1094958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain usernames of pro non-compliance users \n",
    "noncompliers = dd.groupby('username')['opinion'].value_counts(normalize=True).unstack(fill_value = 0).apply(lambda x: x[0] > x[1], axis = 1)\n",
    "# pass as list \n",
    "noncompliers = list(noncompliers[noncompliers].index)\n",
    "# select data of non-compliers only \n",
    "to_kmeans = dd[[True if i in noncompliers else False for i in dd['username']]]\n",
    "# select Friuli-related keywords \n",
    "fvg = ['Trieste', 'Friuli', 'Trieste, Friuli-Venezia Giulia', 'FVG']\n",
    "# Normalize the substrings by removing non-alphanumeric characters and converting to lowercase\n",
    "fvg_normalized = [re.sub(r'\\W+', '', s).lower() for s in fvg]\n",
    "# Function to check if any fvg substring is in the original string\n",
    "def contains_fvg(s):\n",
    "    normalized_s = re.sub(r'\\W+', '', s).lower()\n",
    "    return any(sub in normalized_s for sub in fvg_normalized)\n",
    "# Apply the function to each string\n",
    "results = [contains_fvg(s) for s in to_kmeans['location'].value_counts().index]\n",
    "# add location column\n",
    "fvg_loc = to_kmeans['location'].value_counts().index[results]\n",
    "# change name of location column for better interpretability \n",
    "to_kmeans['location'] = ['in-friuli' if i in fvg_loc else 'out-friuli' for i in to_kmeans['location']]\n",
    "# remove nans \n",
    "to_kmeans['type_1'] = to_kmeans['type_1'].fillna('')\n",
    "to_kmeans['Meta'] = to_kmeans['Meta'].fillna('')\n",
    "# define helepr \n",
    "def stacker(colname, to_drop = ''):\n",
    "    result = to_kmeans.groupby('username')[colname].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    return result.drop(to_drop, axis=1)\n",
    "# process data to obtain summary for users \n",
    "tokm_gr = pd.concat([\n",
    "           stacker('location', to_drop = 'out-friuli'), # otbain locaiton summary \n",
    "           to_kmeans.groupby('username')['retweet_count'].agg(np.median),  # obtain median retweet count\n",
    "           stacker('best_result', to_drop = ['joy', 'fear']), # obtain frequency of anger and sadness \n",
    "           stacker('Meta', to_drop = ['', 'noise', 'genova port joins the protests', 'port blocking', \n",
    "                                     'port workers and workers union', 'prostests and Rome', \n",
    "                                      'protests and fasism', 'protests and media journalists', \n",
    "                                      'protests cause covid outbreak', 'puzzer', 'square of trieste ',\n",
    "                                      'troubled protests and possible issues with protests ',\n",
    "                                      'port workers announce port block', 'foreign hidden influences on the port'\n",
    "                                     ]), # remove non-relevant topics \n",
    "           to_kmeans.groupby('username').agg(n=('type_1', lambda x: len(list(x)))) # add tweet count \n",
    "          ], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0560fe66",
   "metadata": {},
   "source": [
    "## For elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb190bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def find_optimal_clusters(data, max_k):\n",
    "    inertia = []\n",
    "    for k in range(2, max_k+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(2, max_k+1), inertia, marker='o')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.show()\n",
    "\n",
    "# Call the functions with your dataframe\n",
    "find_optimal_clusters(tokm_gr, 10)  # Check clusters from 2 to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd348a",
   "metadata": {},
   "source": [
    "## Fit k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after elbow method \n",
    "k = 4\n",
    "# select kmeans model\n",
    "kmeans = KMeans(n_clusters=k, random_state=88)\n",
    "# select scaler for standardization \n",
    "scaler = StandardScaler()\n",
    "# normalize data \n",
    "df_scaled = scaler.fit_transform(tokm_gr)\n",
    "# fit k-means and pass centroids to dataframe \n",
    "to_mplt = pd.DataFrame(kmeans.fit(df_scaled).cluster_centers_, columns = tokm_gr.columns, index = ['centroid ' + str(i) for i in range(k)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d15299f",
   "metadata": {},
   "source": [
    "# Difference between average pro non-compliance and pro compliance user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf13539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge non compliers list to tweet feed \n",
    "tokm_gr = pd.merge(dd, noncompliers.rename('non-complier') , on = 'username')\n",
    "# add location information again \n",
    "tokm_gr['location'] = ['in-friuli' if i in fvg_loc else 'out-friuli' for i in tokm_gr['location']]\n",
    "# select non-relevant topics \n",
    "to_drop = ['noise', 'genova port joins the protests', 'port blocking', 'port workers and workers union', \n",
    "           'prostests and Rome', 'protests and fasism', 'protests and media journalists', \n",
    "           'protests cause covid outbreak', 'puzzer', 'square of trieste ',\n",
    "           'troubled protests and possible issues with protests ', 'port workers announce port block', \n",
    "           'foreign hidden influences on the port']\n",
    "to_diff = pd.concat([\n",
    "    # median reteweet count \n",
    "    tokm_gr.groupby(['username', 'non-complier'])['retweet_count'].agg(np.median), \n",
    "    # average location\n",
    "    tokm_gr.groupby(['username', 'non-complier'])['location'].agg(lambda x: x.value_counts()[0] / len(x)),\n",
    "    # frequency of emotion displayed in tweeets \n",
    "    tokm_gr.groupby(['username', 'non-complier'])['best_result'].value_counts(normalize = True).unstack(level = 2, fill_value = 0).drop(['joy', 'fear'], axis = 1),\n",
    "    # frequency of topics found in tweets \n",
    "    tokm_gr.groupby(['username', 'non-complier'])['Meta'].value_counts(normalize = True).unstack(level = 2, fill_value = 0).drop(to_drop, axis = 1),\n",
    "    # add n of published tweets \n",
    "    tokm_gr.groupby(['username', 'non-complier']).agg(n=('type_1', lambda x: len(list(x))))\n",
    "    ], axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
