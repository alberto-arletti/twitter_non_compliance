{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3207d971",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd59d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tweets from file \n",
    "df = pd.read_csv('/Users/.../csv_files/merged.csv')\n",
    "# remove retweets\n",
    "db = df[[not i for i in df['is_retweet']]]\n",
    "# add date-like format \n",
    "db['created_at'] = pd.to_datetime(db['created_at'])\n",
    "# remove tweets that got no likes or retweets or comments \n",
    "db = db[(db[['reply_count', 'retweet_count', 'like_count']].sum(axis = 1) != 0)]\n",
    "def is_whitespace_or_url_only(tweet):\n",
    "    # Regex to match URLs\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # Remove all URLs from the tweet\n",
    "    tweet_without_urls = re.sub(url_pattern, '', tweet)\n",
    "    # Check if the remaining tweet is only whitespace\n",
    "    return tweet_without_urls.strip() != ''\n",
    "db = db[db['fulltext'].apply(is_whitespace_or_url_only)]\n",
    "# remove caps which might get identified as a topic \n",
    "db['fulltext'] = db['fulltext'].str.lower()\n",
    "# news outlet list \n",
    "news_outlets = ['il_piccolo', 'infoitinterno', 'TgrRaiFVG', 'Open_gol', 'TRIESTE_news', 'Ansa_Fvg', 'Corriere', \n",
    "                'MediasetTgcom24', 'La7tv', 'localteamtv', 'ComunediTrieste', 'Telequattro', 'messveneto', \n",
    "               'Gazzettino', 'Radio1Rai', 'tempoweb', 'Telefriuli1', 'SkyTG24', 'Agenzia_Ansa', 'ilgiornale',\n",
    "               'fanpage', 'IlFriuli', 'DomaniGiornale', 'DiscoverTrieste', 'ImolaOggi', 'repubblica', 'ilfoglio_it',\n",
    "               'RaCapodistria', 'informatrieste', 'Agenzia_Italia', 'UdineseTV', 'VisioneTv', 'neXtquotidiano', \n",
    "               'fattoquotidiano', 'ilmessaggeroit', 'HuffPostItalia', 'Roma_H_24', 'RaiNews', 'RadioRadioWeb', 'RadioGenova',\n",
    "               'localteamtv', 'byoblu', 'RadioSavana', 'fanpage', 'MediasetTgcom24', 'LaStampa', 'TgLa7',\n",
    "               'Libero_official']\n",
    "# remove from data \n",
    "db = db[[True if not i in news_outlets else False for i in db['username']]]\n",
    "# drop text duplicates \n",
    "db = db.loc[db['fulltext'].drop_duplicates().index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5258ad35",
   "metadata": {},
   "source": [
    "# Topic Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a841ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import openai\n",
    "from bertopic.representation import OpenAI\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the API key from the text file\n",
    "key_location = '/Users/.../openAI_key.txt'\n",
    "with open(key_location, 'r') as file:\n",
    "    api_key = file.read().strip()  # Remove any leading/trailing whitespace\n",
    "    \n",
    "# Create the OpenAI client with the API key\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "representation_model = OpenAI(client, model=\"gpt-3.5-turbo\", chat=True)\n",
    "# select embedding model \n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "# select BERTopic hyperparameters \n",
    "topic_model = BERTopic(embedding_model=embedding_model,\n",
    "                       representation_model=representation_model, \n",
    "                       verbose=True,\n",
    "                       min_topic_size=70,\n",
    "                       calculate_probabilities = True)\n",
    "# estimate model\n",
    "topics, probs = topic_model.fit_transform(db['fulltext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4602149c",
   "metadata": {},
   "source": [
    "# Import meta-topic classification from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02fedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2.service_account import Credentials\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a58394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load credentials \n",
    "scopes = [\n",
    "    'https://www.googleapis.com/auth/spreadsheets',\n",
    "    'https://www.googleapis.com/auth/drive'\n",
    "]\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    'credentials.json',\n",
    "    scopes=scopes)\n",
    "# access google drive API\n",
    "gc = gspread.authorize(credentials)\n",
    "# open spreadsheet \n",
    "spreadsheet = gc.open(\"twitter_topics_reduced\")\n",
    "# Open the worksheet by its name\n",
    "worksheet = spreadsheet.worksheet('meta')\n",
    "# Get all the data from the worksheet\n",
    "meta_topics = worksheet.get_all_values()\n",
    "# pass to dataframe correctly \n",
    "meta_topics = pd.DataFrame(meta_topics[1:], columns = meta_topics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309465a",
   "metadata": {},
   "source": [
    "# Create composite dataset with added opinion and topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d49c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty topic\n",
    "meta_topics = meta_topics[meta_topics['Topic'] != '']\n",
    "# change type \n",
    "meta_topics['Topic'] = meta_topics['Topic'].astype(int)\n",
    "# merge meta-topics with predicted topics from data\n",
    "stage1 = pd.merge(pd.Series(topic_model.topics_, name = 'Topic'), meta_topics, on = 'Topic', how = 'left')\n",
    "# append topic probabilities and meta-topic to main dataframe\n",
    "stage2 = pd.concat([db.reset_index(), stage1, pd.DataFrame(topic_model.probabilities_)], axis = 1)\n",
    "# drop count column\n",
    "stage2 = stage2.drop('Count', axis = 1)\n",
    "# load predicted opinion file \n",
    "opinion = pd.read_csv('/Users/..../csv_files/opinion.csv')\n",
    "# remove added columns \n",
    "opinion = opinion[['0', '1']]\n",
    "# change name \n",
    "opinion.columns = ['support', 'neutral or against']\n",
    "# take original dataset \n",
    "stage3 = df[df['is_retweet'] == False].reset_index()\n",
    "# concat with opinion\n",
    "stage4 = pd.concat([stage3, opinion], axis = 1)\n",
    "# merge opinion with topic \n",
    "dd = pd.merge(stage4, stage2, on = 'index', how = 'outer')\n",
    "# change columns to string format \n",
    "dd.columns = [str(i) for i in dd.columns]\n",
    "# remove doubled columns\n",
    "columns_to_keep = [col for col in dd.columns if not col.endswith('_y')]\n",
    "dd = dd[columns_to_keep]\n",
    "dd.columns = dd.columns.str.replace('_x$', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a3171",
   "metadata": {},
   "source": [
    "# Prevalence of toipics depending on opinion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add explicit column for opinion to dataframe (better name)\n",
    "dd['opinion'] = dd[['support', 'neutral or against']].apply(np.argmax, axis = 1)\n",
    "# group by topic for each opinion\n",
    "topic_opinion = dd.groupby('opinion')['Topic'].agg(list).apply(lambda x: pd.Series(x).value_counts()).transpose()[1:]\n",
    "# normalized to sum 1\n",
    "stage1 = topic_opinion.div(topic_opinion.apply(sum))\n",
    "# merge with meta-topics dataframe\n",
    "to_mplt = pd.merge(stage1, meta_topics, left_index = True, right_on = 'Topic')\n",
    "# aggregate for meta topic depending on opinion \n",
    "to_plot = pd.concat([to_mplt.groupby('Meta')[0].agg(sum), to_mplt.groupby('Meta')[1].agg(sum)], axis = 1)\n",
    "# drop unwanted topics \n",
    "to_plot = to_plot.drop('noise')\n",
    "# obtain differenced\n",
    "to_plot['diff'] = to_plot[0] - to_plot[1]\n",
    "# sort \n",
    "to_plot = to_plot.sort_values('diff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2559108c",
   "metadata": {},
   "source": [
    "# For explorative k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccef3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain usernames of pro non-compliance users \n",
    "noncompliers = dd.groupby('username')['opinion'].value_counts(normalize=True).unstack(fill_value = 0).apply(lambda x: x[0] > x[1], axis = 1)\n",
    "# pass as list \n",
    "noncompliers = list(noncompliers[noncompliers].index)\n",
    "# select data of non-compliers only \n",
    "to_kmeans = dd[[True if i in noncompliers else False for i in dd['username']]]\n",
    "# select Friuli-related keywords \n",
    "fvg = ['Trieste', 'Friuli', 'Trieste, Friuli-Venezia Giulia', 'FVG']\n",
    "# Normalize the substrings by removing non-alphanumeric characters and converting to lowercase\n",
    "fvg_normalized = [re.sub(r'\\W+', '', s).lower() for s in fvg]\n",
    "# Function to check if any fvg substring is in the original string\n",
    "def contains_fvg(s):\n",
    "    normalized_s = re.sub(r'\\W+', '', s).lower()\n",
    "    return any(sub in normalized_s for sub in fvg_normalized)\n",
    "# Apply the function to each string\n",
    "results = [contains_fvg(s) for s in to_kmeans['location'].value_counts().index]\n",
    "# add location column\n",
    "fvg_loc = to_kmeans['location'].value_counts().index[results]\n",
    "# change name of location column for better interpretability \n",
    "to_kmeans['location'] = ['in-friuli' if i in fvg_loc else 'out-friuli' for i in to_kmeans['location']]\n",
    "# remove nans \n",
    "to_kmeans['type_1'] = to_kmeans['type_1'].fillna('')\n",
    "to_kmeans['Meta'] = to_kmeans['Meta'].fillna('')\n",
    "# define helepr \n",
    "def stacker(colname, to_drop = ''):\n",
    "    result = to_kmeans.groupby('username')[colname].value_counts(normalize=True).unstack(fill_value=0)\n",
    "    return result.drop(to_drop, axis=1)\n",
    "# process data to obtain summary for users \n",
    "tokm_gr = pd.concat([\n",
    "           stacker('location', to_drop = 'out-friuli'), # otbain locaiton summary \n",
    "           to_kmeans.groupby('username')['retweet_count'].agg(np.median),  # obtain median retweet count\n",
    "           stacker('best_result', to_drop = ['joy', 'fear']), # obtain frequency of anger and sadness \n",
    "           stacker('Meta', to_drop = ['', 'noise', 'genova port joins the protests', 'port blocking', \n",
    "                                     'port workers and workers union', 'prostests and Rome', \n",
    "                                      'protests and fasism', 'protests and media journalists', \n",
    "                                      'protests cause covid outbreak', 'puzzer', 'square of trieste ',\n",
    "                                      'troubled protests and possible issues with protests ',\n",
    "                                      'port workers announce port block', 'foreign hidden influences on the port'\n",
    "                                     ]), # remove non-relevant topics \n",
    "           to_kmeans.groupby('username').agg(n=('type_1', lambda x: len(list(x)))) # add tweet count \n",
    "          ], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a2649",
   "metadata": {},
   "source": [
    "## For elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee5870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def find_optimal_clusters(data, max_k):\n",
    "    inertia = []\n",
    "    for k in range(2, max_k+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(2, max_k+1), inertia, marker='o')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.show()\n",
    "\n",
    "# Call the functions with your dataframe\n",
    "find_optimal_clusters(tokm_gr, 10)  # Check clusters from 2 to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8186f",
   "metadata": {},
   "source": [
    "## Fit k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after elbow method \n",
    "k = 4\n",
    "# select kmeans model\n",
    "kmeans = KMeans(n_clusters=k, random_state=88)\n",
    "# select scaler for standardization \n",
    "scaler = StandardScaler()\n",
    "# normalize data \n",
    "df_scaled = scaler.fit_transform(tokm_gr)\n",
    "# fit k-means and pass centroids to dataframe \n",
    "to_mplt = pd.DataFrame(kmeans.fit(df_scaled).cluster_centers_, columns = tokm_gr.columns, index = ['centroid ' + str(i) for i in range(k)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aadc78",
   "metadata": {},
   "source": [
    "# Difference between average pro non-compliance and pro compliance user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge non compliers list to tweet feed \n",
    "tokm_gr = pd.merge(dd, noncompliers.rename('non-complier') , on = 'username')\n",
    "# add location information again \n",
    "tokm_gr['location'] = ['in-friuli' if i in fvg_loc else 'out-friuli' for i in tokm_gr['location']]\n",
    "# select non-relevant topics \n",
    "to_drop = ['noise', 'genova port joins the protests', 'port blocking', 'port workers and workers union', \n",
    "           'prostests and Rome', 'protests and fasism', 'protests and media journalists', \n",
    "           'protests cause covid outbreak', 'puzzer', 'square of trieste ',\n",
    "           'troubled protests and possible issues with protests ', 'port workers announce port block', \n",
    "           'foreign hidden influences on the port']\n",
    "to_diff = pd.concat([\n",
    "    # median reteweet count \n",
    "    tokm_gr.groupby(['username', 'non-complier'])['retweet_count'].agg(np.median), \n",
    "    # average location\n",
    "    tokm_gr.groupby(['username', 'non-complier'])['location'].agg(lambda x: x.value_counts()[0] / len(x)),\n",
    "    # frequency of emotion displayed in tweeets \n",
    "    tokm_gr.groupby(['username', 'non-complier'])['best_result'].value_counts(normalize = True).unstack(level = 2, fill_value = 0).drop(['joy', 'fear'], axis = 1),\n",
    "    # frequency of topics found in tweets \n",
    "    tokm_gr.groupby(['username', 'non-complier'])['Meta'].value_counts(normalize = True).unstack(level = 2, fill_value = 0).drop(to_drop, axis = 1),\n",
    "    # add n of published tweets \n",
    "    tokm_gr.groupby(['username', 'non-complier']).agg(n=('type_1', lambda x: len(list(x))))\n",
    "    ], axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
